{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "#import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "#from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from keras.utils.layer_utils import get_source_inputs\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "/media/nlplab/hdd3/Laith/postdoc_2022_January_server/Balanced_Main/HARD-Arabic-Dataset-master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HARD = pd.read_csv(\"balanced-reviews.txt\", sep=\"\\t\", header=0,encoding='utf-16')\n",
    "\n",
    "DATA_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "df_HARD = df_HARD[[\"review\",\"rating\"]]  # we are interested in rating and review only\n",
    "df_HARD.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "print(df_HARD[LABEL_COLUMN].value_counts())\n",
    "# code rating as +ve if > 3, -ve if less, no 3s in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HARD2 = pd.read_csv(\"balanced-reviews.txt\", sep=\"\\t\", header=0,encoding='utf-16')\n",
    "\n",
    "df_HARD2 = df_HARD2[[\"review\",\"rating\"]]  # we are interested in rating and review only\n",
    "df_HARD2.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "\n",
    "# code rating as +ve if > 3, -ve if less, no 3s in dataset\n",
    "\n",
    "hard_map = {\n",
    "    5: 'POS',\n",
    "    4: 'POS',\n",
    "    2: 'NEG',\n",
    "    1: 'NEG'\n",
    "}\n",
    "\n",
    "df_HARD2[LABEL_COLUMN] = df_HARD2[LABEL_COLUMN].apply(lambda x: hard_map[x])\n",
    "\n",
    "print(df_HARD2[LABEL_COLUMN].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HARD.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def deNoise(text):\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ،    | #commas\n",
    "                             ؛    | # commas\n",
    "                             \\*    | #stars\n",
    "                             \\.   | # full stop\n",
    "                             \\( | \\) | #parenthesis\n",
    "                             ,,     | #double commas\n",
    "                             ,      | #single comma\n",
    "                             ;       |#sim col\n",
    "                             :    | #col\n",
    "                             _    | #underscore \n",
    "                             \\=  | #eq\n",
    "                             \\\\\\ | #comm\n",
    "                             \\/   | #comme\n",
    "                              -   | #dash\n",
    "                              ـ     # Tatwil/Kashida\n",
    "                              \n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    return text\n",
    "def normalizeArabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"[https]\", ' ', text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HARD[DATA_COLUMN]=[deNoise(i)for i in df_HARD[DATA_COLUMN]] \n",
    "df_HARD[DATA_COLUMN]=[normalizeArabic(i)for i in df_HARD[DATA_COLUMN]]  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HARD2[DATA_COLUMN]=[deNoise(i)for i in df_HARD2[DATA_COLUMN]] \n",
    "df_HARD2[DATA_COLUMN]=[normalizeArabic(i)for i in df_HARD2[DATA_COLUMN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(df_HARD, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train2,test2 = train_test_split(df_HARD2, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TRAIN= list(train['text'])+ list(train2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TEST= list(test['text'])+ list(test2['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token= Tokenizer(filters=('!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'))\n",
    "token.fit_on_texts(list(TOTAL_TRAIN)  + list(TOTAL_TEST))\n",
    "#tokenizer.fit_on_texts(list(train_data['text']) + list(test_data['text']))\n",
    "vocab= len(token.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "token= Tokenizer(filters=('!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'))\n",
    "token.fit_on_texts(list(train2['text'])  + list(test2['text']))\n",
    "#tokenizer.fit_on_texts(list(train_data['text']) + list(test_data['text']))\n",
    "vocab2= len(token.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=50\n",
    "train['text'] =token.texts_to_sequences(train['text'])\n",
    "test['text'] =token.texts_to_sequences(test['text'])\n",
    "\n",
    "train2['text'] =token.texts_to_sequences(train2['text'])\n",
    "test2['text'] =token.texts_to_sequences(test2['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(list(train['text']), maxlen=max_len, padding='post', truncating='post')\n",
    "x_test = pad_sequences(list(test['text']), maxlen=max_len, padding='post', truncating='post')\n",
    "#------\n",
    "x_train2 = pad_sequences(list(train2['text']), maxlen=max_len, padding='post', truncating='post')\n",
    "x_test2 = pad_sequences(list(test2['text']), maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kutilities # to install this module use :  pip install keras-utilities\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kutilities.helpers.data_preparation import  categories_to_onehot,labels_to_categories, get_labels_to_categories_map, get_class_weights2, onehot_to_categories\n",
    "#TASK4 class\n",
    "y_train= labels_to_categories(train['label'])\n",
    "y_test= labels_to_categories(test['label'])\n",
    "y_train=categories_to_onehot(y_train)\n",
    "y_test=categories_to_onehot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2= labels_to_categories(train2['label'])\n",
    "y_test2= labels_to_categories(test2['label'])\n",
    "y_train2=categories_to_onehot(y_train2)\n",
    "y_test2=categories_to_onehot(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TransformerLayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size,nb_layers,hidden_dim,filter_size,num_heads,dropout,maximum_position_encoding=200):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.filter_size= filter_size\n",
    "        self.nb_layers = nb_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.maximum_position_encoding = maximum_position_encoding\n",
    "        super(SentenceEncoder,self).__init__()\n",
    "    def build(self,input_shape):\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, \n",
    "                                        self.hidden_dim,\n",
    "                                        )\n",
    "        self.encoder = EncoderSubnetwork(nb_layers=self.nb_layers,\n",
    "                                         hidden_dim=self.hidden_dim,\n",
    "                                         filter_size=self.filter_size,\n",
    "                                         num_heads=self.num_heads,dropout=self.dropout) \n",
    "        self.outputPool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.pos_encoding = positional_encoding(self.maximum_position_encoding,\n",
    "                                            self.hidden_dim)\n",
    "\n",
    "        super(SentenceEncoder,self).build(input_shape)\n",
    "    def call(self,input_x,training=False,return_attention_weights=False):\n",
    "        src_embedding = self.embedding_layer(input_x)\n",
    "        seq_len = tf.shape(input_x)[1]\n",
    "        src_embedding *= tf.math.sqrt(tf.cast(self.hidden_dim, tf.float32))\n",
    "        src_embedding += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "\n",
    "        attention_bias = get_padding_bias(input_x)\n",
    "        \n",
    "        sentence_encode = self.encoder(src_embedding,\n",
    "                                       attention_bias,\n",
    "                                       training,\n",
    "                                       return_attention_weights=return_attention_weights)\n",
    "        if return_attention_weights:\n",
    "            sentence_encode, attention_weights = sentence_encode\n",
    "        sentence_encode = self.outputPool(sentence_encode)\n",
    "        return sentence_encode if not return_attention_weights else [sentence_encode, attention_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tf.keras.layers.Input(shape=(None,))\n",
    "src_input2 = tf.keras.layers.Input(shape=(None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = SentenceEncoder(vocab,nb_layers=7,hidden_dim=128,filter_size=256,num_heads=4,dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding =sentence_encoder(src_input)\n",
    "sentence_embedding2 =sentence_encoder(src_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_1 = tf.keras.layers.Dense(4, activation='softmax')(sentence_embedding)\n",
    "dense_layer_2 = tf.keras.layers.Dense(2, activation='sigmoid')(sentence_embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=src_input, outputs=dense_layer_1)\n",
    "model2 = tf.keras.models.Model(inputs=src_input2, outputs=dense_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "learning_rate = CustomSchedule(128)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(8):\n",
    "    # Check if x is even\n",
    "    if x % 2 == 0:\n",
    "        print('4class')\n",
    "        history= model.fit(x_train, y_train, batch_size=64, epochs=1, verbose=1, validation_data=(x_test,y_test))\n",
    "\n",
    "    else:\n",
    "        print('2class')\n",
    "        history_2=model2.fit(x_train2, y_train2, batch_size=64, epochs=1, verbose=1, validation_data=(x_test2,y_test2)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from kutilities.helpers.data_preparation import onehot_to_categories\n",
    "from sklearn import metrics\n",
    "from collections import Counter \n",
    "y_pre=model.predict(x_test)\n",
    "y_pre=onehot_to_categories(y_pre)\n",
    "y_test= onehot_to_categories(y_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_pre,y_test))\n",
    "print(\"f1:\",metrics.f1_score(y_pre,y_test,average='macro'))\n",
    "print(\"P:\",metrics.recall_score(y_pre,y_test,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"f1:\",metrics.f1_score(y_pre,y_test,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=categories_to_onehot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('Laith': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "58f04523d68df69db2fd157688f8b182e5291f0014859e20f95b8f5973a38544"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
